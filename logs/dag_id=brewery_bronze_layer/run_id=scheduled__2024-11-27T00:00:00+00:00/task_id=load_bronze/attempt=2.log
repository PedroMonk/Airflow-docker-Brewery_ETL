[2024-11-28T00:09:24.844+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-11-28T00:09:24.861+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: brewery_bronze_layer.load_bronze scheduled__2024-11-27T00:00:00+00:00 [queued]>
[2024-11-28T00:09:24.870+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: brewery_bronze_layer.load_bronze scheduled__2024-11-27T00:00:00+00:00 [queued]>
[2024-11-28T00:09:24.871+0000] {taskinstance.py:2866} INFO - Starting attempt 2 of 2
[2024-11-28T00:09:24.883+0000] {taskinstance.py:2889} INFO - Executing <Task(PythonOperator): load_bronze> on 2024-11-27 00:00:00+00:00
[2024-11-28T00:09:24.889+0000] {standard_task_runner.py:72} INFO - Started process 86 to run task
[2024-11-28T00:09:24.893+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'brewery_bronze_layer', 'load_bronze', 'scheduled__2024-11-27T00:00:00+00:00', '--job-id', '40', '--raw', '--subdir', 'DAGS_FOLDER/openbrewery_dag.py', '--cfg-path', '/tmp/tmpn6aojjcb']
[2024-11-28T00:09:24.898+0000] {standard_task_runner.py:105} INFO - Job 40: Subtask load_bronze
[2024-11-28T00:09:24.959+0000] {task_command.py:467} INFO - Running <TaskInstance: brewery_bronze_layer.load_bronze scheduled__2024-11-27T00:00:00+00:00 [running]> on host c27aa55db975
[2024-11-28T00:09:25.073+0000] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='brewery_bronze_layer' AIRFLOW_CTX_TASK_ID='load_bronze' AIRFLOW_CTX_EXECUTION_DATE='2024-11-27T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-11-27T00:00:00+00:00'
[2024-11-28T00:09:25.075+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-11-28T00:09:25.088+0000] {load_bronze_module.py:73} INFO - Iniciando a tarefa load_bronze
[2024-11-28T00:09:25.229+0000] {load_bronze_module.py:83} INFO - Recebidos 8355 registros para carregar na camada Bronze
[2024-11-28T00:09:25.230+0000] {load_bronze_module.py:40} INFO - Criando sess√£o Spark
[2024-11-28T00:09:25.238+0000] {load_bronze_module.py:129} ERROR - Erro ao salvar dados na camada Bronze: 'Variable AWS_ACCESS_KEY_ID does not exist'
[2024-11-28T00:09:25.238+0000] {taskinstance.py:3311} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/opt/airflow/scripts/load_bronze_module.py", line 86, in load_bronze
    spark = create_spark_session()
  File "/opt/airflow/scripts/load_bronze_module.py", line 43, in create_spark_session
    .config("spark.hadoop.fs.s3a.access.key", Variable.get("AWS_ACCESS_KEY_ID")) \
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/variable.py", line 145, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable AWS_ACCESS_KEY_ID does not exist'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 417, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/scripts/load_bronze_module.py", line 132, in load_bronze
    spark.stop()
UnboundLocalError: local variable 'spark' referenced before assignment
[2024-11-28T00:09:25.246+0000] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=brewery_bronze_layer, task_id=load_bronze, run_id=scheduled__2024-11-27T00:00:00+00:00, execution_date=20241127T000000, start_date=20241128T000924, end_date=20241128T000925
[2024-11-28T00:09:25.256+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-11-28T00:09:25.256+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 40 for task load_bronze (local variable 'spark' referenced before assignment; 86)
Traceback (most recent call last):
  File "/opt/airflow/scripts/load_bronze_module.py", line 86, in load_bronze
    spark = create_spark_session()
  File "/opt/airflow/scripts/load_bronze_module.py", line 43, in create_spark_session
    .config("spark.hadoop.fs.s3a.access.key", Variable.get("AWS_ACCESS_KEY_ID")) \
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/variable.py", line 145, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable AWS_ACCESS_KEY_ID does not exist'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3005, in _run_raw_task
    return _run_raw_task(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3159, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3183, in _execute_task
    return _execute_task(self, context, task_orig)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 417, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/scripts/load_bronze_module.py", line 132, in load_bronze
    spark.stop()
UnboundLocalError: local variable 'spark' referenced before assignment
[2024-11-28T00:09:25.268+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2024-11-28T00:09:25.286+0000] {taskinstance.py:3895} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-11-28T00:09:25.288+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-11-28T00:38:42.553+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-11-28T00:38:42.566+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: brewery_bronze_layer.load_bronze scheduled__2024-11-27T00:00:00+00:00 [queued]>
[2024-11-28T00:38:42.572+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: brewery_bronze_layer.load_bronze scheduled__2024-11-27T00:00:00+00:00 [queued]>
[2024-11-28T00:38:42.572+0000] {taskinstance.py:2866} INFO - Starting attempt 2 of 2
[2024-11-28T00:38:42.582+0000] {taskinstance.py:2889} INFO - Executing <Task(PythonOperator): load_bronze> on 2024-11-27 00:00:00+00:00
[2024-11-28T00:38:42.586+0000] {standard_task_runner.py:72} INFO - Started process 47 to run task
[2024-11-28T00:38:42.589+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'brewery_bronze_layer', 'load_bronze', 'scheduled__2024-11-27T00:00:00+00:00', '--job-id', '46', '--raw', '--subdir', 'DAGS_FOLDER/openbrewery_dag.py', '--cfg-path', '/tmp/tmptzi7oeg7']
[2024-11-28T00:38:42.591+0000] {standard_task_runner.py:105} INFO - Job 46: Subtask load_bronze
[2024-11-28T00:38:42.629+0000] {task_command.py:467} INFO - Running <TaskInstance: brewery_bronze_layer.load_bronze scheduled__2024-11-27T00:00:00+00:00 [running]> on host 34dd061d127a
[2024-11-28T00:38:42.698+0000] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='brewery_bronze_layer' AIRFLOW_CTX_TASK_ID='load_bronze' AIRFLOW_CTX_EXECUTION_DATE='2024-11-27T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-11-27T00:00:00+00:00'
[2024-11-28T00:38:42.699+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-11-28T00:38:42.722+0000] {load_bronze_module.py:88} INFO - Iniciando a tarefa load_bronze
[2024-11-28T00:38:42.847+0000] {load_bronze_module.py:98} INFO - Recebidos 8355 registros para carregar na camada Bronze
[2024-11-28T00:38:42.847+0000] {load_bronze_module.py:62} ERROR - Erro ao criar sess√£o Spark: name 'os' is not defined
[2024-11-28T00:38:42.847+0000] {load_bronze_module.py:144} ERROR - Erro ao salvar dados na camada Bronze: name 'os' is not defined
[2024-11-28T00:38:42.848+0000] {taskinstance.py:3311} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/opt/airflow/scripts/load_bronze_module.py", line 101, in load_bronze
    spark = create_spark_session()
  File "/opt/airflow/scripts/load_bronze_module.py", line 42, in create_spark_session
    aws_access_key = os.getenv("AWS_ACCESS_KEY_ID")
NameError: name 'os' is not defined

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 417, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/scripts/load_bronze_module.py", line 147, in load_bronze
    spark.stop()
UnboundLocalError: local variable 'spark' referenced before assignment
[2024-11-28T00:38:42.856+0000] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=brewery_bronze_layer, task_id=load_bronze, run_id=scheduled__2024-11-27T00:00:00+00:00, execution_date=20241127T000000, start_date=20241128T003842, end_date=20241128T003842
[2024-11-28T00:38:42.866+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-11-28T00:38:42.867+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 46 for task load_bronze (local variable 'spark' referenced before assignment; 47)
Traceback (most recent call last):
  File "/opt/airflow/scripts/load_bronze_module.py", line 101, in load_bronze
    spark = create_spark_session()
  File "/opt/airflow/scripts/load_bronze_module.py", line 42, in create_spark_session
    aws_access_key = os.getenv("AWS_ACCESS_KEY_ID")
NameError: name 'os' is not defined

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3005, in _run_raw_task
    return _run_raw_task(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3159, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3183, in _execute_task
    return _execute_task(self, context, task_orig)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 417, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/scripts/load_bronze_module.py", line 147, in load_bronze
    spark.stop()
UnboundLocalError: local variable 'spark' referenced before assignment
[2024-11-28T00:38:42.883+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2024-11-28T00:38:42.897+0000] {taskinstance.py:3895} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-11-28T00:38:42.899+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-11-28T02:56:59.551+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-11-28T02:56:59.566+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: brewery_bronze_layer.load_bronze scheduled__2024-11-27T00:00:00+00:00 [queued]>
[2024-11-28T02:56:59.573+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: brewery_bronze_layer.load_bronze scheduled__2024-11-27T00:00:00+00:00 [queued]>
[2024-11-28T02:56:59.573+0000] {taskinstance.py:2866} INFO - Starting attempt 2 of 2
[2024-11-28T02:56:59.586+0000] {taskinstance.py:2889} INFO - Executing <Task(PythonOperator): load_bronze> on 2024-11-27 00:00:00+00:00
[2024-11-28T02:56:59.592+0000] {standard_task_runner.py:72} INFO - Started process 712 to run task
[2024-11-28T02:56:59.597+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'brewery_bronze_layer', 'load_bronze', 'scheduled__2024-11-27T00:00:00+00:00', '--job-id', '74', '--raw', '--subdir', 'DAGS_FOLDER/openbrewery_dag.py', '--cfg-path', '/tmp/tmptq55tefv']
[2024-11-28T02:56:59.599+0000] {standard_task_runner.py:105} INFO - Job 74: Subtask load_bronze
[2024-11-28T02:56:59.648+0000] {task_command.py:467} INFO - Running <TaskInstance: brewery_bronze_layer.load_bronze scheduled__2024-11-27T00:00:00+00:00 [running]> on host a76fd60aefd9
[2024-11-28T02:56:59.711+0000] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='brewery_bronze_layer' AIRFLOW_CTX_TASK_ID='load_bronze' AIRFLOW_CTX_EXECUTION_DATE='2024-11-27T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-11-27T00:00:00+00:00'
[2024-11-28T02:56:59.712+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-11-28T02:56:59.723+0000] {load_bronze_module.py:104} INFO - Iniciando a tarefa load_bronze
[2024-11-28T02:56:59.879+0000] {load_bronze_module.py:115} INFO - Recebidos 8355 registros para carregar na camada Bronze
[2024-11-28T02:57:02.564+0000] {load_bronze_module.py:62} INFO - Sess√£o Spark criada com sucesso
[2024-11-28T02:57:02.565+0000] {load_bronze_module.py:119} INFO - Sess√£o Spark criada com sucesso
[2024-11-28T02:57:02.565+0000] {load_bronze_module.py:139} INFO - Criando DataFrame com Spark
[2024-11-28T02:57:04.900+0000] {load_bronze_module.py:142} INFO - DataFrame criado com sucesso
[2024-11-28T02:57:04.901+0000] {load_bronze_module.py:144} INFO - Verificando dados que n√£o atendem ao padr√£o
[2024-11-28T02:57:11.845+0000] {load_bronze_module.py:151} ERROR - Existem 2328 registros com longitude inv√°lida.
[2024-11-28T02:57:11.855+0000] {load_bronze_module.py:172} ERROR - Erro ao salvar dados na camada Bronze: 'Variable S3_BRONZE_PATH does not exist'
[2024-11-28T02:57:12.687+0000] {load_bronze_module.py:177} INFO - Sess√£o Spark finalizada e tarefa load_bronze conclu√≠da
[2024-11-28T02:57:12.688+0000] {taskinstance.py:3311} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 417, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/scripts/load_bronze_module.py", line 153, in load_bronze
    invalid_records_path = f"{Variable.get('S3_BRONZE_PATH')}/errors/date={kwargs['execution_date'].strftime('%Y-%m-%d')}"
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/variable.py", line 145, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable S3_BRONZE_PATH does not exist'
[2024-11-28T02:57:12.696+0000] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=brewery_bronze_layer, task_id=load_bronze, run_id=scheduled__2024-11-27T00:00:00+00:00, execution_date=20241127T000000, start_date=20241128T025659, end_date=20241128T025712
[2024-11-28T02:57:12.705+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-11-28T02:57:12.706+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 74 for task load_bronze ('Variable S3_BRONZE_PATH does not exist'; 712)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3005, in _run_raw_task
    return _run_raw_task(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3159, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3183, in _execute_task
    return _execute_task(self, context, task_orig)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 417, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/scripts/load_bronze_module.py", line 153, in load_bronze
    invalid_records_path = f"{Variable.get('S3_BRONZE_PATH')}/errors/date={kwargs['execution_date'].strftime('%Y-%m-%d')}"
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/variable.py", line 145, in get
    raise KeyError(f"Variable {key} does not exist")
KeyError: 'Variable S3_BRONZE_PATH does not exist'
[2024-11-28T02:57:12.734+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2024-11-28T02:57:12.749+0000] {taskinstance.py:3895} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-11-28T02:57:12.750+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-11-28T03:13:03.155+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-11-28T03:13:03.168+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: brewery_bronze_layer.load_bronze scheduled__2024-11-27T00:00:00+00:00 [queued]>
[2024-11-28T03:13:03.173+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: brewery_bronze_layer.load_bronze scheduled__2024-11-27T00:00:00+00:00 [queued]>
[2024-11-28T03:13:03.174+0000] {taskinstance.py:2866} INFO - Starting attempt 2 of 2
[2024-11-28T03:13:03.183+0000] {taskinstance.py:2889} INFO - Executing <Task(PythonOperator): load_bronze> on 2024-11-27 00:00:00+00:00
[2024-11-28T03:13:03.187+0000] {standard_task_runner.py:72} INFO - Started process 62 to run task
[2024-11-28T03:13:03.190+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'brewery_bronze_layer', 'load_bronze', 'scheduled__2024-11-27T00:00:00+00:00', '--job-id', '87', '--raw', '--subdir', 'DAGS_FOLDER/openbrewery_dag.py', '--cfg-path', '/tmp/tmp7r6zjv8i']
[2024-11-28T03:13:03.191+0000] {standard_task_runner.py:105} INFO - Job 87: Subtask load_bronze
[2024-11-28T03:13:03.227+0000] {task_command.py:467} INFO - Running <TaskInstance: brewery_bronze_layer.load_bronze scheduled__2024-11-27T00:00:00+00:00 [running]> on host d875d93d15db
[2024-11-28T03:13:03.291+0000] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='brewery_bronze_layer' AIRFLOW_CTX_TASK_ID='load_bronze' AIRFLOW_CTX_EXECUTION_DATE='2024-11-27T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-11-27T00:00:00+00:00'
[2024-11-28T03:13:03.292+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-11-28T03:13:03.315+0000] {load_bronze_module.py:108} INFO - Iniciando a tarefa load_bronze
[2024-11-28T03:13:03.439+0000] {load_bronze_module.py:119} INFO - Recebidos 8355 registros para carregar na camada Bronze
[2024-11-28T03:13:16.754+0000] {local_task_job_runner.py:127} ERROR - Received SIGTERM. Terminating subprocesses
[2024-11-28T03:13:16.757+0000] {process_utils.py:132} INFO - Sending 15 to group 62. PIDs of all processes in the group: [64, 62]
[2024-11-28T03:13:16.757+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 62
[2024-11-28T03:13:16.758+0000] {taskinstance.py:3093} ERROR - Received SIGTERM. Terminating subprocesses.
[2024-11-28T03:13:16.771+0000] {taskinstance.py:3311} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 417, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/scripts/load_bronze_module.py", line 122, in load_bronze
    logger.info("Sess√£o Spark criada com sucesso")
  File "/opt/airflow/scripts/load_bronze_module.py", line 56, in create_spark_session
    .appName("LoadBronzeOpenBreweryDB") \
  File "/home/airflow/.local/lib/python3.9/site-packages/pyspark/sql/session.py", line 477, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
  File "/home/airflow/.local/lib/python3.9/site-packages/pyspark/context.py", line 512, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "/home/airflow/.local/lib/python3.9/site-packages/pyspark/context.py", line 198, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/home/airflow/.local/lib/python3.9/site-packages/pyspark/context.py", line 432, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
  File "/home/airflow/.local/lib/python3.9/site-packages/pyspark/java_gateway.py", line 103, in launch_gateway
    time.sleep(0.1)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3095, in signal_handler
    raise AirflowTaskTerminated("Task received SIGTERM signal")
airflow.exceptions.AirflowTaskTerminated: Task received SIGTERM signal
[2024-11-28T03:13:16.776+0000] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=brewery_bronze_layer, task_id=load_bronze, run_id=scheduled__2024-11-27T00:00:00+00:00, execution_date=20241127T000000, start_date=20241128T031303, end_date=20241128T031316
[2024-11-28T03:13:16.787+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-11-28T03:13:16.810+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=64, status='terminated', started='03:13:02') (64) terminated with exit code None
[2024-11-28T03:13:16.810+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=62, status='terminated', exitcode=2, started='03:13:02') (62) terminated with exit code 2
[2024-11-28T03:13:16.811+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 143
[2024-11-28T03:13:16.826+0000] {taskinstance.py:3895} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-11-28T03:13:20.747+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-11-28T03:13:20.763+0000] {taskinstance.py:2603} INFO - Dependencies not met for <TaskInstance: brewery_bronze_layer.load_bronze scheduled__2024-11-27T00:00:00+00:00 [failed]>, dependency 'Task Instance State' FAILED: Task is in the 'failed' state.
[2024-11-28T03:13:20.765+0000] {local_task_job_runner.py:166} INFO - Task is not able to be run
[2024-11-28T03:21:44.126+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-11-28T03:21:44.142+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: brewery_bronze_layer.load_bronze scheduled__2024-11-27T00:00:00+00:00 [queued]>
[2024-11-28T03:21:44.149+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: brewery_bronze_layer.load_bronze scheduled__2024-11-27T00:00:00+00:00 [queued]>
[2024-11-28T03:21:44.149+0000] {taskinstance.py:2866} INFO - Starting attempt 2 of 2
[2024-11-28T03:21:44.161+0000] {taskinstance.py:2889} INFO - Executing <Task(PythonOperator): load_bronze> on 2024-11-27 00:00:00+00:00
[2024-11-28T03:21:44.167+0000] {standard_task_runner.py:72} INFO - Started process 536 to run task
[2024-11-28T03:21:44.171+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'brewery_bronze_layer', 'load_bronze', 'scheduled__2024-11-27T00:00:00+00:00', '--job-id', '96', '--raw', '--subdir', 'DAGS_FOLDER/openbrewery_dag.py', '--cfg-path', '/tmp/tmpikgegsg0']
[2024-11-28T03:21:44.173+0000] {standard_task_runner.py:105} INFO - Job 96: Subtask load_bronze
[2024-11-28T03:21:44.218+0000] {task_command.py:467} INFO - Running <TaskInstance: brewery_bronze_layer.load_bronze scheduled__2024-11-27T00:00:00+00:00 [running]> on host f0c2c00071e1
[2024-11-28T03:21:44.278+0000] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='brewery_bronze_layer' AIRFLOW_CTX_TASK_ID='load_bronze' AIRFLOW_CTX_EXECUTION_DATE='2024-11-27T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-11-27T00:00:00+00:00'
[2024-11-28T03:21:44.279+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-11-28T03:21:44.294+0000] {load_bronze_module.py:108} INFO - Iniciando a tarefa load_bronze
[2024-11-28T03:21:44.415+0000] {load_bronze_module.py:119} INFO - Recebidos 8355 registros para carregar na camada Bronze
[2024-11-28T03:21:48.695+0000] {load_bronze_module.py:66} INFO - Sess√£o Spark criada com sucesso
[2024-11-28T03:21:48.696+0000] {load_bronze_module.py:123} INFO - Sess√£o Spark criada com sucesso
[2024-11-28T03:21:48.696+0000] {load_bronze_module.py:143} INFO - Criando DataFrame com Spark
[2024-11-28T03:21:53.522+0000] {load_bronze_module.py:146} INFO - DataFrame criado com sucesso
[2024-11-28T03:21:53.523+0000] {load_bronze_module.py:148} INFO - Verificando dados que n√£o atendem ao padr√£o
[2024-11-28T03:22:05.007+0000] {local_task_job_runner.py:346} WARNING - State of this instance has been externally set to None. Terminating instance.
[2024-11-28T03:22:05.013+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-11-28T03:22:05.018+0000] {process_utils.py:132} INFO - Sending 15 to group 536. PIDs of all processes in the group: [538, 686, 750, 765, 768, 773, 777, 781, 784, 790, 793, 798, 802, 804, 809, 700, 701, 708, 712, 716, 720, 724, 728, 731, 734, 735, 739, 536]
[2024-11-28T03:22:05.034+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 536
[2024-11-28T03:22:05.037+0000] {taskinstance.py:3093} ERROR - Received SIGTERM. Terminating subprocesses.
[2024-11-28T03:22:05.384+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=773, status='terminated', started='03:21:56') (773) terminated with exit code None
[2024-11-28T03:22:05.385+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=731, status='terminated', started='03:21:55') (731) terminated with exit code None
[2024-11-28T03:22:05.389+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=765, status='terminated', started='03:21:56') (765) terminated with exit code None
[2024-11-28T03:22:05.390+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=712, status='terminated', started='03:21:55') (712) terminated with exit code None
[2024-11-28T03:22:05.390+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=734, status='terminated', started='03:21:55') (734) terminated with exit code None
[2024-11-28T03:22:05.391+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=735, status='terminated', started='03:21:55') (735) terminated with exit code None
[2024-11-28T03:22:05.391+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=728, status='terminated', started='03:21:55') (728) terminated with exit code None
[2024-11-28T03:22:05.406+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=777, status='terminated', started='03:21:56') (777) terminated with exit code None
[2024-11-28T03:22:05.461+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=750, status='terminated', started='03:21:55') (750) terminated with exit code None
[2024-11-28T03:22:05.462+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=739, status='terminated', started='03:21:55') (739) terminated with exit code None
[2024-11-28T03:22:05.463+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=802, status='terminated', started='03:21:56') (802) terminated with exit code None
[2024-11-28T03:22:05.464+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=793, status='terminated', started='03:21:56') (793) terminated with exit code None
[2024-11-28T03:22:05.516+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=701, status='terminated', started='03:21:55') (701) terminated with exit code None
[2024-11-28T03:22:05.517+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=724, status='terminated', started='03:21:55') (724) terminated with exit code None
[2024-11-28T03:22:05.517+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=716, status='terminated', started='03:21:55') (716) terminated with exit code None
[2024-11-28T03:22:05.518+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=790, status='terminated', started='03:21:56') (790) terminated with exit code None
[2024-11-28T03:22:05.518+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=686, status='terminated', started='03:21:54') (686) terminated with exit code None
[2024-11-28T03:22:05.570+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=798, status='terminated', started='03:21:56') (798) terminated with exit code None
[2024-11-28T03:22:05.571+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=809, status='terminated', started='03:21:56') (809) terminated with exit code None
[2024-11-28T03:22:05.571+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=768, status='terminated', started='03:21:56') (768) terminated with exit code None
[2024-11-28T03:22:05.571+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=804, status='terminated', started='03:21:56') (804) terminated with exit code None
[2024-11-28T03:22:05.572+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=708, status='terminated', started='03:21:55') (708) terminated with exit code None
[2024-11-28T03:22:05.572+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=720, status='terminated', started='03:21:55') (720) terminated with exit code None
[2024-11-28T03:22:05.572+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=700, status='terminated', started='03:21:55') (700) terminated with exit code None
[2024-11-28T03:22:05.573+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=781, status='terminated', started='03:21:56') (781) terminated with exit code None
[2024-11-28T03:22:05.705+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=784, status='terminated', started='03:21:56') (784) terminated with exit code None
[2024-11-28T03:22:05.811+0000] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/airflow/scripts/load_bronze_module.py", line 153, in load_bronze
    invalid_longitudes = df_spark.filter(~col("is_valid_longitude")).count()
  File "/home/airflow/.local/lib/python3.9/site-packages/pyspark/sql/dataframe.py", line 1193, in count
    return int(self._jdf.count())
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.9/socket.py", line 716, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3095, in signal_handler
    raise AirflowTaskTerminated("Task received SIGTERM signal")
airflow.exceptions.AirflowTaskTerminated: Task received SIGTERM signal

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.9/socket.py", line 716, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer
[2024-11-28T03:22:05.817+0000] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-11-28T03:22:05.818+0000] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/airflow/scripts/load_bronze_module.py", line 153, in load_bronze
    invalid_longitudes = df_spark.filter(~col("is_valid_longitude")).count()
  File "/home/airflow/.local/lib/python3.9/site-packages/pyspark/sql/dataframe.py", line 1193, in count
    return int(self._jdf.count())
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.9/socket.py", line 716, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3095, in signal_handler
    raise AirflowTaskTerminated("Task received SIGTERM signal")
airflow.exceptions.AirflowTaskTerminated: Task received SIGTERM signal

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.9/socket.py", line 716, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-11-28T03:22:05.820+0000] {logging_mixin.py:190} WARNING - /home/***/.local/lib/python3.9/site-packages/pyspark/context.py:654 RuntimeWarning: Unable to cleanly shutdown Spark JVM process. It is possible that the process has crashed, been killed or may also be in a zombie state.
[2024-11-28T03:22:06.116+0000] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-11-28T03:22:06.117+0000] {taskinstance.py:3311} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/opt/airflow/scripts/load_bronze_module.py", line 153, in load_bronze
    invalid_longitudes = df_spark.filter(~col("is_valid_longitude")).count()
  File "/home/airflow/.local/lib/python3.9/site-packages/pyspark/sql/dataframe.py", line 1193, in count
    return int(self._jdf.count())
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.9/socket.py", line 716, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3095, in signal_handler
    raise AirflowTaskTerminated("Task received SIGTERM signal")
airflow.exceptions.AirflowTaskTerminated: Task received SIGTERM signal

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 417, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/scripts/load_bronze_module.py", line 180, in load_bronze
    spark.stop()
  File "/home/airflow/.local/lib/python3.9/site-packages/pyspark/sql/session.py", line 1602, in stop
    self._jvm.SparkSession.clearDefaultSession()
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/java_gateway.py", line 1712, in __getattr__
    answer = self._gateway_client.send_command(
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [Errno 111] Connection refused
[2024-11-28T03:22:06.126+0000] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=brewery_bronze_layer, task_id=load_bronze, run_id=scheduled__2024-11-27T00:00:00+00:00, execution_date=20241127T000000, start_date=20241128T032144, end_date=20241128T032206
[2024-11-28T03:22:06.139+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-11-28T03:22:06.139+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 96 for task load_bronze ((psycopg2.errors.ForeignKeyViolation) insert or update on table "task_fail" violates foreign key constraint "task_fail_ti_fkey"
DETAIL:  Key (dag_id, task_id, run_id, map_index)=(brewery_bronze_layer, load_bronze, scheduled__2024-11-27T00:00:00+00:00, -1) is not present in table "task_instance".

[SQL: INSERT INTO task_fail (task_id, dag_id, run_id, map_index, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(run_id)s, %(map_index)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 'load_bronze', 'dag_id': 'brewery_bronze_layer', 'run_id': 'scheduled__2024-11-27T00:00:00+00:00', 'map_index': -1, 'start_date': datetime.datetime(2024, 11, 28, 3, 21, 44, 142854, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2024, 11, 28, 3, 22, 6, 125424, tzinfo=Timezone('UTC')), 'duration': 21}]
(Background on this error at: https://sqlalche.me/e/14/gkpj); 536)
Traceback (most recent call last):
  File "/opt/airflow/scripts/load_bronze_module.py", line 153, in load_bronze
    invalid_longitudes = df_spark.filter(~col("is_valid_longitude")).count()
  File "/home/airflow/.local/lib/python3.9/site-packages/pyspark/sql/dataframe.py", line 1193, in count
    return int(self._jdf.count())
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.9/socket.py", line 716, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3095, in signal_handler
    raise AirflowTaskTerminated("Task received SIGTERM signal")
airflow.exceptions.AirflowTaskTerminated: Task received SIGTERM signal

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3159, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3183, in _execute_task
    return _execute_task(self, context, task_orig)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 417, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/scripts/load_bronze_module.py", line 180, in load_bronze
    spark.stop()
  File "/home/airflow/.local/lib/python3.9/site-packages/pyspark/sql/session.py", line 1602, in stop
    self._jvm.SparkSession.clearDefaultSession()
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/java_gateway.py", line 1712, in __getattr__
    answer = self._gateway_client.send_command(
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.ForeignKeyViolation: insert or update on table "task_fail" violates foreign key constraint "task_fail_ti_fkey"
DETAIL:  Key (dag_id, task_id, run_id, map_index)=(brewery_bronze_layer, load_bronze, scheduled__2024-11-27T00:00:00+00:00, -1) is not present in table "task_instance".


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3005, in _run_raw_task
    return _run_raw_task(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 336, in _run_raw_task
    ti.handle_failure(e, test_mode, context, session=session)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3428, in handle_failure
    _handle_failure(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/api_internal/internal_api_call.py", line 166, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1243, in _handle_failure
    TaskInstance.save_to_db(failure_context["ti"], session)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/api_internal/internal_api_call.py", line 166, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 94, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3400, in save_to_db
    session.flush()
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 3589, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/orm/persistence.py", line 1238, in _emit_insert_statements
    result = connection._execute_20(
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2134, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1910, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.ForeignKeyViolation) insert or update on table "task_fail" violates foreign key constraint "task_fail_ti_fkey"
DETAIL:  Key (dag_id, task_id, run_id, map_index)=(brewery_bronze_layer, load_bronze, scheduled__2024-11-27T00:00:00+00:00, -1) is not present in table "task_instance".

[SQL: INSERT INTO task_fail (task_id, dag_id, run_id, map_index, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(run_id)s, %(map_index)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 'load_bronze', 'dag_id': 'brewery_bronze_layer', 'run_id': 'scheduled__2024-11-27T00:00:00+00:00', 'map_index': -1, 'start_date': datetime.datetime(2024, 11, 28, 3, 21, 44, 142854, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2024, 11, 28, 3, 22, 6, 125424, tzinfo=Timezone('UTC')), 'duration': 21}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2024-11-28T03:22:06.171+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=538, status='terminated', started='03:21:44') (538) terminated with exit code None
[2024-11-28T03:22:06.171+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=536, status='terminated', exitcode=1, started='03:21:44') (536) terminated with exit code 1
[2024-11-28T03:28:40.857+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-11-28T03:28:40.872+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: brewery_bronze_layer.load_bronze scheduled__2024-11-27T00:00:00+00:00 [queued]>
[2024-11-28T03:28:40.879+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: brewery_bronze_layer.load_bronze scheduled__2024-11-27T00:00:00+00:00 [queued]>
[2024-11-28T03:28:40.879+0000] {taskinstance.py:2866} INFO - Starting attempt 2 of 2
[2024-11-28T03:28:40.889+0000] {taskinstance.py:2889} INFO - Executing <Task(PythonOperator): load_bronze> on 2024-11-27 00:00:00+00:00
[2024-11-28T03:28:40.895+0000] {standard_task_runner.py:72} INFO - Started process 2350 to run task
[2024-11-28T03:28:40.898+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'brewery_bronze_layer', 'load_bronze', 'scheduled__2024-11-27T00:00:00+00:00', '--job-id', '102', '--raw', '--subdir', 'DAGS_FOLDER/openbrewery_dag.py', '--cfg-path', '/tmp/tmpoau6oe_p']
[2024-11-28T03:28:40.900+0000] {standard_task_runner.py:105} INFO - Job 102: Subtask load_bronze
[2024-11-28T03:28:40.946+0000] {task_command.py:467} INFO - Running <TaskInstance: brewery_bronze_layer.load_bronze scheduled__2024-11-27T00:00:00+00:00 [running]> on host f0c2c00071e1
[2024-11-28T03:28:41.041+0000] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='brewery_bronze_layer' AIRFLOW_CTX_TASK_ID='load_bronze' AIRFLOW_CTX_EXECUTION_DATE='2024-11-27T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-11-27T00:00:00+00:00'
[2024-11-28T03:28:41.042+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-11-28T03:28:41.059+0000] {load_bronze_module.py:111} INFO - Iniciando a tarefa load_bronze
[2024-11-28T03:28:41.203+0000] {load_bronze_module.py:122} INFO - Recebidos 8355 registros para carregar na camada Bronze
[2024-11-28T03:28:45.256+0000] {load_bronze_module.py:69} INFO - Sess√£o Spark criada com sucesso
[2024-11-28T03:28:45.257+0000] {load_bronze_module.py:126} INFO - Sess√£o Spark criada com sucesso
[2024-11-28T03:28:45.257+0000] {load_bronze_module.py:146} INFO - Criando DataFrame com Spark
[2024-11-28T03:28:47.914+0000] {load_bronze_module.py:149} INFO - DataFrame criado com sucesso
[2024-11-28T03:28:47.915+0000] {load_bronze_module.py:151} INFO - Verificando dados que n√£o atendem ao padr√£o
[2024-11-28T03:29:04.330+0000] {load_bronze_module.py:158} ERROR - Existem 2328 registros com longitude inv√°lida.
[2024-11-28T03:29:04.341+0000] {load_bronze_module.py:162} INFO - Salvando registros inv√°lidos em: s3a://breweris-etl-prod-sp/bronze/errors/date=2024-11-27/invalid_longitudes.json
[2024-11-28T03:29:06.746+0000] {load_bronze_module.py:179} ERROR - Erro ao salvar dados na camada Bronze: An error occurred while calling o79.json.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 13) (f0c2c00071e1 executor driver): java.lang.NoSuchMethodError: 'void org.apache.hadoop.util.SemaphoredDelegatingExecutor.<init>(com.google.common.util.concurrent.ListeningExecutorService, int, boolean)'
	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:772)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:771)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.NoSuchMethodError: 'void org.apache.hadoop.util.SemaphoredDelegatingExecutor.<init>(com.google.common.util.concurrent.ListeningExecutorService, int, boolean)'
	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:772)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 more

[2024-11-28T03:26:11.619+0000] {load_bronze_module.py:184} INFO - Sess√£o Spark finalizada e tarefa load_bronze conclu√≠da
[2024-11-28T03:26:11.620+0000] {taskinstance.py:3311} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 417, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/scripts/load_bronze_module.py", line 163, in load_bronze
    df_invalid.write.mode('append').json(f"s3a://{os.getenv('S3_BUCKET')}/{invalid_records_path}/invalid_longitudes.json")
  File "/home/airflow/.local/lib/python3.9/site-packages/pyspark/sql/readwriter.py", line 1593, in json
    self._jwrite.json(path)
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/airflow/.local/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o79.json.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 13) (f0c2c00071e1 executor driver): java.lang.NoSuchMethodError: 'void org.apache.hadoop.util.SemaphoredDelegatingExecutor.<init>(com.google.common.util.concurrent.ListeningExecutorService, int, boolean)'
	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:772)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:771)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.NoSuchMethodError: 'void org.apache.hadoop.util.SemaphoredDelegatingExecutor.<init>(com.google.common.util.concurrent.ListeningExecutorService, int, boolean)'
	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:772)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 more

[2024-11-28T03:26:11.677+0000] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=brewery_bronze_layer, task_id=load_bronze, run_id=scheduled__2024-11-27T00:00:00+00:00, execution_date=20241127T000000, start_date=20241128T032840, end_date=20241128T032611
[2024-11-28T03:29:08.141+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-11-28T03:29:08.141+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 102 for task load_bronze (An error occurred while calling o79.json.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 13) (f0c2c00071e1 executor driver): java.lang.NoSuchMethodError: 'void org.apache.hadoop.util.SemaphoredDelegatingExecutor.<init>(com.google.common.util.concurrent.ListeningExecutorService, int, boolean)'
	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:772)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:771)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.NoSuchMethodError: 'void org.apache.hadoop.util.SemaphoredDelegatingExecutor.<init>(com.google.common.util.concurrent.ListeningExecutorService, int, boolean)'
	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:772)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 more
; 2350)
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3005, in _run_raw_task
    return _run_raw_task(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3159, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 3183, in _execute_task
    return _execute_task(self, context, task_orig)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 417, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
  File "/opt/airflow/scripts/load_bronze_module.py", line 163, in load_bronze
    df_invalid.write.mode('append').json(f"s3a://{os.getenv('S3_BUCKET')}/{invalid_records_path}/invalid_longitudes.json")
  File "/home/airflow/.local/lib/python3.9/site-packages/pyspark/sql/readwriter.py", line 1593, in json
    self._jwrite.json(path)
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/airflow/.local/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.9/site-packages/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o79.json.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 13) (f0c2c00071e1 executor driver): java.lang.NoSuchMethodError: 'void org.apache.hadoop.util.SemaphoredDelegatingExecutor.<init>(com.google.common.util.concurrent.ListeningExecutorService, int, boolean)'
	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:772)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:771)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.NoSuchMethodError: 'void org.apache.hadoop.util.SemaphoredDelegatingExecutor.<init>(com.google.common.util.concurrent.ListeningExecutorService, int, boolean)'
	at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:772)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)
	at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonOutputWriter.scala:47)
	at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:81)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)
	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	... 1 more

[2024-11-28T03:29:08.282+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2024-11-28T03:29:08.299+0000] {taskinstance.py:3895} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-11-28T03:29:08.300+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2024-11-28T04:18:46.205+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-11-28T04:18:46.219+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: brewery_bronze_layer.load_bronze scheduled__2024-11-27T00:00:00+00:00 [queued]>
[2024-11-28T04:18:46.225+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: brewery_bronze_layer.load_bronze scheduled__2024-11-27T00:00:00+00:00 [queued]>
[2024-11-28T04:18:46.225+0000] {taskinstance.py:2866} INFO - Starting attempt 2 of 2
[2024-11-28T04:18:46.235+0000] {taskinstance.py:2889} INFO - Executing <Task(PythonOperator): load_bronze> on 2024-11-27 00:00:00+00:00
[2024-11-28T04:18:46.240+0000] {standard_task_runner.py:72} INFO - Started process 2078 to run task
[2024-11-28T04:18:46.243+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'brewery_bronze_layer', 'load_bronze', 'scheduled__2024-11-27T00:00:00+00:00', '--job-id', '138', '--raw', '--subdir', 'DAGS_FOLDER/openbrewery_dag.py', '--cfg-path', '/tmp/tmp_x2cjkyc']
[2024-11-28T04:18:46.245+0000] {standard_task_runner.py:105} INFO - Job 138: Subtask load_bronze
[2024-11-28T04:18:46.286+0000] {task_command.py:467} INFO - Running <TaskInstance: brewery_bronze_layer.load_bronze scheduled__2024-11-27T00:00:00+00:00 [running]> on host 2683cbf29485
[2024-11-28T04:18:46.373+0000] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='brewery_bronze_layer' AIRFLOW_CTX_TASK_ID='load_bronze' AIRFLOW_CTX_EXECUTION_DATE='2024-11-27T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-11-27T00:00:00+00:00'
[2024-11-28T04:18:46.374+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-11-28T04:18:46.388+0000] {load_bronze_module.py:109} INFO - Iniciando a tarefa load_bronze
[2024-11-28T04:18:46.520+0000] {load_bronze_module.py:128} INFO - Recebidos 8355 registros para carregar na camada Bronze
[2024-11-28T04:18:46.521+0000] {load_bronze_module.py:43} INFO - Iniciando cria√ß√£o da sess√£o Spark
[2024-11-28T04:18:54.176+0000] {load_bronze_module.py:67} INFO - Sess√£o Spark criada com sucesso
[2024-11-28T04:18:54.176+0000] {load_bronze_module.py:132} INFO - Sess√£o Spark criada com sucesso
[2024-11-28T04:18:54.177+0000] {load_bronze_module.py:152} INFO - Criando DataFrame com Spark
[2024-11-28T04:18:57.415+0000] {load_bronze_module.py:155} INFO - DataFrame criado com sucesso
[2024-11-28T04:18:57.415+0000] {load_bronze_module.py:157} INFO - Verificando dados que n√£o atendem ao padr√£o
[2024-11-28T04:19:34.592+0000] {load_bronze_module.py:164} ERROR - Existem 2328 registros com longitude inv√°lida.
[2024-11-28T04:19:34.601+0000] {load_bronze_module.py:168} INFO - Salvando registros inv√°lidos em: s3a://breweris-etl-prod-sp/bronze/errors/date=2024-11-27/invalid_longitudes.json
[2024-11-28T04:20:21.901+0000] {load_bronze_module.py:179} INFO - Salvando dados no S3 em: s3a://breweris-etl-prod-sp/bronze/date=2024-11-27
[2024-11-28T04:20:32.773+0000] {load_bronze_module.py:183} INFO - Dados salvos com sucesso na camada Bronze: bronze/date=2024-11-27
[2024-11-28T04:20:33.522+0000] {load_bronze_module.py:190} INFO - Sess√£o Spark finalizada e tarefa load_bronze conclu√≠da
[2024-11-28T04:20:33.530+0000] {python.py:240} INFO - Done. Returned value was: None
[2024-11-28T04:20:33.542+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-11-28T04:20:33.542+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=brewery_bronze_layer, task_id=load_bronze, run_id=scheduled__2024-11-27T00:00:00+00:00, execution_date=20241127T000000, start_date=20241128T041846, end_date=20241128T042033
[2024-11-28T04:20:33.595+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2024-11-28T04:20:33.614+0000] {taskinstance.py:3895} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-11-28T04:20:33.616+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
